{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8271a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36d04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7aefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: jinja2 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/isita/Downloads/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39140a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let\n",
      "us\n",
      "tokenize\n"
     ]
    }
   ],
   "source": [
    "string = nlp('let us tokenize')\n",
    "for token in string:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6ae4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let VERB ROOT let\n",
      "us PRON nsubj we\n",
      "tokenize VERB ccomp tokenize\n"
     ]
    }
   ],
   "source": [
    "string = nlp('let us tokenize')\n",
    "for token in string:\n",
    "    print(token.text,token.pos_,token.dep_,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a94cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fbc4e94af40>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fbc4e959cc0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fbc4e6eedc0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fbc4e573fc0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fbc4e9b0b00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fbc4e6ee880>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e93d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0384ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = nlp('let us learn how to use spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ed7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let VERB ROOT\n",
      "us PRON nsubj\n",
      "learn VERB ccomp\n",
      "how ADV advmod\n",
      "to PART aux\n",
      "use VERB xcomp\n",
      "spacy NOUN dobj\n"
     ]
    }
   ],
   "source": [
    "for token in string2:\n",
    "    print(token.text,token.pos_,token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46b91b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string2[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7376867",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('First Sentence.Second Sentence.Third Sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48adc5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence.\n",
      "Second Sentence.\n",
      "Third Sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7e470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'The ice cream costs $4.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59bc0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "ice\n",
      "cream\n",
      "costs\n",
      "$\n",
      "4.5\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ea1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The|ice|cream|costs|$|4.5|"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text,end ='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d37c2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n",
      "MONEY\n",
      "Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for entity in doc2.ents:\n",
    "    print(entity)\n",
    "    print(entity.label_)\n",
    "    print(str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144f7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ice cream\n"
     ]
    }
   ],
   "source": [
    "for noun in doc2.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afbe9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55bd0c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"aff53c062d274dbabb6060817a69e688-0\" class=\"displacy\" width=\"650\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">ice</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">cream</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">costs</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">4.5</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aff53c062d274dbabb6060817a69e688-0-0\" stroke-width=\"2px\" d=\"M70,102.0 C70,2.0 250.0,2.0 250.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aff53c062d274dbabb6060817a69e688-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,104.0 L62,92.0 78,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aff53c062d274dbabb6060817a69e688-0-1\" stroke-width=\"2px\" d=\"M170,102.0 C170,52.0 245.0,52.0 245.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aff53c062d274dbabb6060817a69e688-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,104.0 L162,92.0 178,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aff53c062d274dbabb6060817a69e688-0-2\" stroke-width=\"2px\" d=\"M270,102.0 C270,52.0 345.0,52.0 345.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aff53c062d274dbabb6060817a69e688-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,104.0 L262,92.0 278,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aff53c062d274dbabb6060817a69e688-0-3\" stroke-width=\"2px\" d=\"M470,102.0 C470,52.0 545.0,52.0 545.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aff53c062d274dbabb6060817a69e688-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,104.0 L462,92.0 478,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aff53c062d274dbabb6060817a69e688-0-4\" stroke-width=\"2px\" d=\"M370,102.0 C370,2.0 550.0,2.0 550.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aff53c062d274dbabb6060817a69e688-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,104.0 L558.0,92.0 542.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc2,style = 'dep',jupyter=True,options={'distance':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdb65580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The ice cream costs $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4.5\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc2,style = 'ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb91386",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f3fa6",
   "metadata": {},
   "source": [
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78703dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aec7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a09f0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e374232",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','ran','runs','easily','fairly','fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1b8e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n",
      "ran\n",
      "run\n",
      "easili\n",
      "fairli\n",
      "fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c72c947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7b24512",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14a49f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n",
      "ran\n",
      "run\n",
      "easili\n",
      "fair\n",
      "fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(stemmer2.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc022b",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a4bb1",
   "metadata": {},
   "source": [
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576caba",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization both generate the foundation sort of the inflected words and therefore the only difference is that stem may not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.\n",
    "\n",
    "Stemming and lemmatization are methods used by search engines and chatbots to analyze the meaning behind a word. Stemming uses the stem of the word, while lemmatization uses the context in which the word is being used. We'll later go into more detailed explanations and examples.  \n",
    "\n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations. \n",
    "\n",
    "Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9edd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
